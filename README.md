# Vehicle Data Pipeline Project

## üìå Project Overview
This project aims to build a fully automated data pipeline using MongoDB, Python, and AWS services. It includes data ingestion, validation, transformation, model training, and deployment using CI/CD on AWS EC2.

---

## üöÄ Project Setup
### 1Ô∏è‚É£ Create Project Template
Run the following command to initialize the project structure:
```bash
python template.py
```

### 2Ô∏è‚É£ Configure Local Package Imports
- Modify `setup.py` and `pyproject.toml` to enable local package imports.
- More details are available in `crashcourse.txt`.

### 3Ô∏è‚É£ Set Up Virtual Environment
```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
```
Verify installation:
```bash
pip list
```

---

## üóÑÔ∏è MongoDB Setup
### 4Ô∏è‚É£ Set Up MongoDB Atlas
1. Sign up on [MongoDB Atlas](https://www.mongodb.com/atlas) and create a new project.
2. Click on **Create a Cluster** ‚Üí Choose **M0 Free Tier** ‚Üí Deploy.
3. Create a **Database User** with a username and password.
4. In **Network Access**, add IP address `0.0.0.0/0` for global access.
5. Get the **MongoDB Connection String** (Driver: Python 3.6+).
6. Save the connection string (replace `<password>` with your actual password).

### 5Ô∏è‚É£ Push Data to MongoDB
1. Create a `notebook` folder and add `mongoDB_demo.ipynb`.
2. Load the dataset and push it to MongoDB.
3. Verify data in **MongoDB Atlas** ‚Üí Database ‚Üí Browse Collection.

---

## üìù Logging, Exception Handling & Notebooks
### 6Ô∏è‚É£ Implement Logging & Exception Handling
- Create `logger.py` and `exception.py`, test them in `demo.py`.
- Add **EDA & Feature Engineering** notebooks.

---

## üì• Data Ingestion
### 7Ô∏è‚É£ Implement Data Ingestion
- Define constants in `constants/__init__.py`.
- Implement MongoDB connection in `configuration/mongo_db_connections.py`.
- Add data retrieval logic in `data_access/proj1_data.py`.
- Define entities in `entity/config_entity.py` and `entity/artifact_entity.py`.
- Implement `components/data_ingestion.py`.
- Run `demo.py` to test.

### 8Ô∏è‚É£ Configure MongoDB Connection URL
```bash
export MONGODB_URL="mongodb+srv://<username>:<password>@cluster.mongodb.net/"
echo $MONGODB_URL
```
(For Windows, set environment variables manually.)

---

## üèóÔ∏è Data Validation, Transformation & Model Training
### 9Ô∏è‚É£ Data Validation
- Complete `utils/main_utils.py` and `config/schema.yaml`.
- Implement validation as done in **Data Ingestion**.

### üîü Data Transformation
- Implement transformation logic.
- Update `estimator.py` in `entity/`.

### 1Ô∏è‚É£1Ô∏è‚É£ Model Training
- Implement training logic in `estimator.py`.
- Train and evaluate the model.

---

## ‚òÅÔ∏è AWS Setup
### 1Ô∏è‚É£2Ô∏è‚É£ AWS IAM & S3 Configuration
- Create an IAM user with `AdministratorAccess`.
- Generate access keys and set environment variables:
  ```bash
  export AWS_ACCESS_KEY_ID="<your-access-key>"
  export AWS_SECRET_ACCESS_KEY="<your-secret-key>"
  ```
- Create an S3 bucket `my-model-mlopsproj`.
- Implement `aws_connection.py` for S3 interaction.

---

## üìä Model Evaluation & Deployment
### 1Ô∏è‚É£3Ô∏è‚É£ Model Evaluation & Pusher
- Implement Model Evaluation & Pusher components.

### 1Ô∏è‚É£4Ô∏è‚É£ Prediction Pipeline & Web App
- Implement `app.py`.
- Add `static` and `templates` directories.

---

## üîÑ CI/CD with Docker & GitHub Actions
### 1Ô∏è‚É£5Ô∏è‚É£ Docker Setup
- Create `Dockerfile` and `.dockerignore`.

### 1Ô∏è‚É£6Ô∏è‚É£ GitHub Actions for CI/CD
- Create `.github/workflows/aws.yaml`.
- Set up **GitHub Secrets**:
  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`
  - `AWS_DEFAULT_REGION`
  - `ECR_REPO`

### 1Ô∏è‚É£7Ô∏è‚É£ Deploy to AWS EC2
- Launch an Ubuntu EC2 instance (T2 Medium, 30GB storage).
- Install Docker:
  ```bash
  curl -fsSL https://get.docker.com -o get-docker.sh
  sudo sh get-docker.sh
  ```
- Set up a **self-hosted runner** on GitHub.

### 1Ô∏è‚É£8Ô∏è‚É£ Expose EC2 Port for App
- Edit **Inbound Rules**:
  - Type: Custom TCP
  - Port: `5080`
  - Source: `0.0.0.0/0`
- Access the app at `http://<EC2-PUBLIC-IP>:5080`.

---

## ‚úÖ Final Steps
- CI/CD pipeline triggers on `git push`.
- Web app is accessible at `/training` route for model training.
- Validate predictions via `app.py`.

---

## üéØ Conclusion
This project provides an end-to-end data pipeline from data ingestion to model deployment using MongoDB, AWS, and CI/CD. üöÄ

